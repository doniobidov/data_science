---
title: 'MA5761 Homework #6'
author: "Doni Obidov"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    theme: journal
    toc: yes
    toc_float: yes
---

```{r}
# Load the packages
library(mvtnorm)
library(DescTools)
library(ggplot2)
library(tidyverse)
```

For homework assignments in this course, you will be provided with a knitted R notebook and the .rmd file used to create it. You are *encouraged* (but not required) to use the .rmd file as a template for your solutions.

## Saving R Objects
The exercises in this assignment will have you performing several extensive Monte Carlo simulations. It should not take an inordinate amount of time to perform all of these simulations (approximately five minutes for each set of requested simulations), but recreating your simulations from scratch is entirely unneccesary. 

After you have created a data frame or other R object that stores the various requested quantities, you should *save* that object to an external file. This can be done with the `save()` function. For instance, suppose that you have a data frame (or matrix, or other R object) storing the output from the various simulations in the first exercise below. In order to avoid having to perform those simulations again (say, if you close your R session and start a new one), you can run the command
```{r, eval = FALSE}
save(my_df, file = "my_saved_df.Rdata")
```
where `my_df` is the name of the data frame (or other R object) and "my_saved_df.Rdata" is the name of the external file that will be written. 

If you wish to load this object back into a new session of R, you can use the command
```{r, eval = FALSE}
load("my_saved_df.Rdata")
```
You should then find that the object `my_df` is present as an object in your R session to be used, viewed, or altered as you wish. ***Be aware of what your working directory is set to***. When saving or loading an object, R will write or open that external file in the present working directory.

When you submit your solutions to this assignment, submit your R program file (or R Markdown file and knitted document), along with each of the saved R objects from each exercise. Your R program should include the code that generates these objects, but also include the `load()` function that loads the saved data from the external file (so that I do not have to run all of your simulations to check your work). 

Ask questions if any of these instructions are unclear.

## Pearson's Sample Correlation

Pearson's sample correlation is defined by
$$R=\frac{\sum_{j=1}^n(X_j-\bar X)(Y_j-\bar Y)}{\sqrt{\sum_{j=1}^n(X_j-\bar X)^2}\sqrt{\sum_{j=1}^n(Y_j-\bar Y)^2}}$$
where $((X_1,Y_1),(X_2,Y_2),\dotsc,(X_n,,Y_n))$ is a sample of bivariate random vectors. It is commonly used to estimate the population correlation coefficient $\rho$, ideally when the random vector is sampled from a bivariate normal population.

You will estimate properties of this statistic for varying sample sizes $n$ and varying values of the population correlation $\rho$. 

### Bivariate Normal Samples
In order to sample *quickly* from a bivariate normal distribution, the `rmvnorm()` function from the `mvtnorm` pacakge is a bit too slow. Suppose that we wish to sample $(X,Y)$ from a bivariate normal distribution with the mean vector and covariance matrix 
$$\boldsymbol{\mu}=\bigl(0, 0\bigr)\quad\text{and}\quad
\boldsymbol{\Sigma}=\begin{pmatrix}1&\rho\\\rho&1\end{pmatrix}$$

Write a function `rbvnorm(n, rho)` which generates a random sample of size $n$ from the bivariate normal distribution with a correlation coefficient of $\rho$. This should implement the following algorithm:

1. Generate a vector $Z_1$ of length $n$ from the standard normal distribution (say, with `rnorm()`)
2. Generate a vector $Z_2$ of length $n$ from the standard normal distribution
3. Return the two vectors $Z_1$ and $\rho Z_1+\sqrt{1-\rho^2}Z_2$ in a matrix of $n$ rows and 2 columns (say, with the `cbind()` function)

```{r}
rbvnorm <- function(n, rho) {
  # Generate vector Z1 from standard normal distribution
  Z1 <- rnorm(n)
  
  # Generate vector Z2 from standard normal distribution
  Z2 <- rnorm(n)

  # Combine into a matrix
  return(cbind(Z1, rho * Z1 + sqrt(1 - rho^2) * Z2))
}
```

### Comparing `rbvnorm` and `rmvnorm` 
Make a comparison of how long it takes to sample $10^4$ instances of the sample correlation coefficient using either `rbvnorm()` or `rmvnorm`. 

Sample $N=10^4$ instances of Pearson's sample correlation calculated from samples of size $n=100$ selected from the bivariate normal distribution with $\rho=0.9$. This can be done most simply with the use of the `replicate()` and `cor()` functions:
```{r, eval = FALSE}
replicate(10^4, cor(rbvnorm(100, rho = 0.9))[1, 2])
```
Note the use of the bracketing with the `cor()` function; passing a matrix to the `cor()` function will result in a correlation matrix, though the correlation we want is in the `[1,2]` index. 

Estimate the amount of time that it takes to generate $10^4$ sample correlations using your `rbvnorm` function. Do this as well when using the `rmvnorm()` function from the `rmvtnorm` pacakge (install this package if you haven't done so before). Note you will need to pass a covariance matrix to the `sigma` argument of this function.

How much longer does it take to generate this sample of Pearson correlations with the `rmvnorm` function? Estimate the ratio of times it takes for these two methods to get a sense of the time savings.

```{r}
# Define the parameters
n <- 100
rho <- 0.9
N <- 10^4

# Measure time for rbvnorm
start_time_rbvnorm <- Sys.time()
correlations_rbvnorm <- replicate(N, cor(rbvnorm(n, rho))[1, 2])
end_time_rbvnorm <- Sys.time()

# Define the covariance matrix
sigma <- matrix(c(1, rho, rho, 1), ncol = 2)

# Measure time for rmvnorm
start_time_rmvnorm <- Sys.time()
correlations_rmvnorm <- replicate(N, cor(rmvnorm(n, sigma = sigma))[1, 2])
end_time_rmvnorm <- Sys.time()

# Calculate the time difference
time_difference_rbvnorm <- as.numeric(end_time_rbvnorm - start_time_rbvnorm)
time_difference_rmvnorm <- as.numeric(end_time_rmvnorm - start_time_rmvnorm)

# Calculate the ratio of times
time_ratio <- time_difference_rmvnorm / time_difference_rbvnorm

# Print the results
cat("Time taken with rbvnorm:", time_difference_rbvnorm, "\n")
cat("Time taken with rmvnorm:", time_difference_rmvnorm, "\n")
cat("Ratio of times (rmvnorm/rbvnorm):", time_ratio, "\n")
```

While you have this sample generated, create a *density* histogram estimate of the sampling distribution (when $n=100$ and $\rho=0.9$). Additionally, overlay a kernel density estimate on top of this histogram. Make a set of brief comments on obvious properties of this sampling distribution.

```{r}
# Density histogram
hist(correlations_rbvnorm, main = "Sampling Distribution of Pearson Correlation (rbvnorm)",
     xlab = "Sample Correlation", col = "lightblue", prob = TRUE)

# Overlay a kernel density estimate
lines(density(correlations_rbvnorm), col = "red")
```

As expected, the mean of this sampling distribution is at 0.9.The distribution is unimodal and skewed to the left. The range is approximately 0.15.

### Properties of Pearson's *R* and Fisher's *Z*
It is known that the sampling distribution of Pearson's $R$ tends to become less variable as $\rho$ approaches $\pm 1$. A popular means of addressing this potential problem is with the use of Fisher's *z*-transformation:
$$Z=\tanh^{-1}(R)$$
where $\tanh$ is the *hyperbolic tangent* function. Note that $\tanh^{-1}$ is implemented in R with the `atanh()` function.

Create a data frame which estimates various properties associated with the sampling distribution of both $R$ and $Z$ for varying values of $n$ and $\rho$. Namely, for $n\in\{20,40,\dotsc,100\}$ and $\rho\in\{0,0.1,0.2,\dotsc,0.9\}$...

- generate $10^4$ instances of Pearson's $R$
- estimate the *mean*, *variance*, and *skewness* of this sampling distribution (recall that the skewness coefficient is implemented with the `Skew()` function from the `DescTools` package, though other implementations exist)
- estimate the *bias*, *standard error*, *mean square error*, and *mean absolute error* of $R$ as a point estimator of $\rho$
- place all of these quantities in a single row of a data frame (or matrix, if you wish)

Repeat this also for Fisher's $Z$. Note that when estimating bias, mean square error, or mean absolute error, you should use `atanh(rho)` the population parameter to be estimated instead of `rho`. 

Your resulting data frame should have columns...

- `Estimator`, taking the value `Pearson R` or `Fisher Z`
- `n` and `rho`
- `Mean`, `Variance`, and `Skew`
- `Bias`, `SE`, `MSE`, and `MAE`

```{r, eval=FALSE}
# Define the values of n and rho
n_values <- seq(20, 100, by = 20)
rho_values <- seq(0, 0.9, by = 0.1)

# Empty data frame to store the results
results_df <- data.frame()

# Perform simulations for Pearson's R
lapply(n_values, function(n) {
  lapply(rho_values, function(rho) {
    set.seed(123)  # Set a seed for reproducibility
    
    # Simulate 10^4 instances of Pearson's R
    R_simulations <- replicate(10^4, cor(rbvnorm(n, rho))[1, 2])
    
    # Calculate the mean, variance, and skewness]
    mean_R <- mean(R_simulations)
    var_R <- var(R_simulations)
    skew_R <- Skew(R_simulations)
    
    # Calculate the bias, standard error, mean square error, and mean absolute error
    bias_R <- mean_R - rho
    se_R <- sd(R_simulations)
    mse_R <- mean((R_simulations - rho)^2)
    mae_R <- mean(abs(R_simulations - rho))
    
    # Create a data frame for results
    R_results <- data.frame(
      Estimator = "Pearson R",
      n = n,
      rho = rho,
      Mean = mean_R,
      Variance = var_R,
      Skew = skew_R,
      Bias = bias_R,
      SE = se_R,
      MSE = mse_R,
      MAE = mae_R
    )
    
    # Append to the results data frame
    results_df <<- rbind(results_df, R_results)
  })
})

# Perform simulations and calculations for Fisher's Z
lapply(n_values, function(n) {
  lapply(rho_values, function(rho) {
    set.seed(123)  # Set a seed for reproducibility
    
    # Simulate 10,000 instances of Fisher's Z
    R_simulations <- replicate(10^4, cor(rbvnorm(n, rho))[1, 2])
    Z_simulations <- atanh(R_simulations)
    
    # Calculate the mean, variance, and skewness
    mean_Z <- mean(Z_simulations)
    var_Z <- var(Z_simulations)
    skew_Z <- Skew(Z_simulations)
    
    # Calculate the bias, standard error, mean square error, and mean absolute error
    bias_Z <- mean_Z - atanh(rho)
    se_Z <- sd(Z_simulations)
    mse_Z <- mean((Z_simulations - atanh(rho))^2)
    mae_Z <- mean(abs(Z_simulations - atanh(rho)))
    
    # Create a data frame for results
    Z_results <- data.frame(
      Estimator = "Fisher Z",
      n = n,
      rho = rho,
      Mean = mean_Z,
      Variance = var_Z,
      Skew = skew_Z,
      Bias = bias_Z,
      SE = se_Z,
      MSE = mse_Z,
      MAE = mae_Z
    )
    
    # Append to the results data frame
    results_df <<- rbind(results_df, Z_results)
  })
})
```

```{r, eval=FALSE}
save(results_df, file = "correlations_df.Rdata")
```

```{r}
load("correlations_df.Rdata")
```

Create graphics to explore the following properties of these two estimators and make brief comments about what patterns are evident. Note that to create graphics which explore the relationship between four variables (such as $n$, $\rho$, some statistical quantity, and the estimator type), you will need to use different aesthetics (such as color, or plotting symbol, or creating several of the same plot with a `facet_wrap`). 

- How is the variance of the sampling distribution of either statistic influence by $\rho$? Pick an $n$ and plot the estimated variance of both estimators across $\rho$. Explain why Fisher's $Z$ transformation is called a ***variance-stabilizing transformation***.

```{r}
# Select rows where n=40
subset_df <- results_df[results_df$n == 40, ]

# Plot
ggplot(subset_df, aes(x = rho, y = Variance, color = Estimator)) +
  geom_line() +
  labs(title = 'Estimated Variance Across Different Values of rho (n=40)',
       x = 'rho',
       y = 'Variance') +
  theme_minimal()
```

**The variance of the Fisher Z estimator is almost a horizontal line. The variance of the Pearson R estimator declines as rho increases.**

- How is the skewness of the sampling distribution of either Pearson's $R$ or Fisher's $Z$ change with respect to $n$ and $\rho$?

```{r}
# Select an estimator
subset_df <- results_df[results_df$Estimator == "Pearson R", ]

# Scatter plot
ggplot(subset_df, aes(x = n, y = rho, color = Variance)) +
  geom_point(size = 6) +
  scale_color_gradient(low = "blue", high = "red") +
  labs(title = "Variance Across Different Values of n and rho (Pearson's R Estimator)",
       x = "n", y = "rho", color = "Variance") +
  theme_minimal()
```

```{r}
# Select an estimator
subset_df <- results_df[results_df$Estimator == "Fisher Z", ]

# Scatter plot
ggplot(subset_df, aes(x = n, y = rho, color = Variance)) +
  geom_point(size = 6) +
  scale_color_gradient(low = "blue", high = "red") +
  labs(title = "Variance Across Different Values of n and rho (Fisher's Z Estimator)",
       x = "n", y = "rho", color = "Variance") +
  theme_minimal()
```

**The variance of Pearson R estimator decreases as either rho or n (or both) increases. On the other hand, the variance of the Fisher Z estimator is invariant to changes in rho; however, larger n tends to decrease the variance**

- How is the bias of Pearson's $R$ or Fisher's $Z$ affected by $n$ and $\rho$? Does one of these estimators tend to overestimate (or underestimate) their respective parameters? Include a horizontal reference line corresponding to 0 bias to make interpretation easier.

```{r}
# Select an estimator
subset_df <- results_df[results_df$Estimator == "Pearson R", ]

# Scatter plot
ggplot(subset_df, aes(x = n, y = rho, color = Bias)) +
  geom_point(size = 6) +
  scale_color_gradient(low = "blue", high = "red") +
  labs(title = "Bias Across Different Values of n and rho (Pearson's R Estimator)",
       x = "n", y = "rho", color = "Variance") +
  theme_minimal()
```

```{r}
# Select an estimator
subset_df <- results_df[results_df$Estimator == "Fisher Z", ]

# Scatter plot
ggplot(subset_df, aes(x = n, y = rho, color = Bias)) +
  geom_point(size = 6) +
  scale_color_gradient(low = "blue", high = "red") +
  labs(title = "Bias Across Different Values of n and rho (Fisher's Z Estimator)",
       x = "n", y = "rho", color = "Variance") +
  theme_minimal()
```

```{r}
# Define the color palette
estimator_colors <- c("Pearson R" = "blue", "Fisher Z" = "red")

# Plot the Bias for each estimator
ggplot(data = subset(results_df, n == 20), aes(x = rho, y = Bias, color = Estimator)) +
  geom_line() +
  geom_point() +
  scale_color_manual(values = estimator_colors) +
  geom_hline(yintercept = 0, linetype = "dashed", color = "black") +
  labs(x = "rho", y = "Bias", title = "Bias vs. rho for n=20") +
  theme_minimal()
```

**As expected, bias is reduced as n becomes larger. In the case of the Fisher Z estimator, the bias linearly increases as rho goes up, whereas in the Pearson Z estimator, bias first goes down then up in a parabolic manner. Moreover, the Fisher Z estimator generally overestimates the population parameter, and the Pearson R estimator underestimates the population parameter.**

- How is $MSE$ and $MAE$ affected by the value of $\rho$? Pick a particular $n$ and plot both of these quantities on the same axes for each of the two estimators.

```{r}
# Filter for a specific n
subset_data <- subset(results_df, n == 20)

# Plot both MSE and MAE on the same axes
ggplot(subset_data, aes(x = rho)) +
  geom_line(aes(y = MSE, color = Estimator, linetype = "MSE")) +
  geom_point(aes(y = MSE, color = Estimator), size = 2) +
  geom_line(aes(y = MAE, color = Estimator, linetype = "MAE")) +
  geom_point(aes(y = MAE, color = Estimator), size = 2) +
  labs(x = "rho", y = "Error", title = paste("MSE and MAE vs. rho for n = 20")) +
  scale_linetype_manual(values = c("solid", "dashed")) +
  theme_minimal()
```

**For both error types, the error remains indifferent to changes in rho in the Fisher Z estimator. In the case of the Pearson R estimator, the error decreases as rho increases. However, it is noteworthy that the error in the Fisher Z estimator consistently appears to be larger than that in the Pearson R estimator.**

## Confidence Intervals on Population Median

We have briefly investigated comparisons of the performance of Student's *t*-interval and Yuen's confidence interval for estimating a population mean. Recall that the trimmed population mean will be equal to the population mean only when the sampled population is symmetric. However, when we are sampling from a symmetric distribution, the mean and median are the same quantity. Consider comparing properties of three different confidence intervals on a population mean/median (only equivalent if the sampled distribution is symmetric):

- Student's *t* interval, as implemented with the `t.test()` function
- Yuen's trimmed mean interval, as implemented with the `YuenTTest()` function from the `DescTools` package
- a confidence interval on a population median, implemented with the `MedianCI()` function from the `DescTools` package

### Slash Distribution

One popular example of a symmetric heavy-tailed distribution is called the ***slash distribution***. Say that $X$ has this slash distribution, with parameter $\delta>0$, if 
$$X=\frac{Z}{U^{1/\delta}}$$
where $Z\sim\mathsf{N}(0,1)$ has a standard normal distribution and $U\sim\mathsf{Uniform}(0,1)$ is uniformly distributed over the unit interval; $Z$ and $U$ are independent of one another.

Write a function `rslash(n, delta)` which generates a sample of size $n$ from the slash distribution with parameter $\delta$. Generate a sample of size $N=10^4$ from this distribution with $\delta=2$ and create a density histogram of it.

```{r}
# Function to generate random numbers the slash distribution
rslash <- function(n, delta) {
  Z <- rnorm(n)
  U <- runif(n)
  X <- Z / U^(1/delta)
  return(X)
}

set.seed(123) # Set the seed for reproducibility

# Generate a sample
N <- 10^4
delta <- 2
slash_sample <- rslash(N, delta)

# Create a data frame
df_slash <- data.frame(Value = slash_sample)

# Density plot
ggplot(df_slash, aes(x = Value)) +
  geom_density(fill = "skyblue", color = "red", alpha = 0.7) +
  labs(title = "Density Plot of Slash Distribution",
       x = "Value", y = "Density") +
  theme_minimal() + 
  xlim(-12, 12)
```

### Comparison of Confidence Intervals

Perform Monte Carlo simulations estimating the properties of these three confidence intervals for varying values of $n$ when sampling from the slash distribution for varying values of $\delta$. Particularly, for each combination of...

- sample size $n$ in the set $\{20, 40, 60, 80, 100\}$
- shape parameter $\delta\in\{1, 1.5, 2, 2.5\}$
- type of confidence interval (Student's *t*, Yuen's *t*, or the median CI)

you should 

- generate $10^4$ samples of size $n$ from the slash distribution with shape parameter $\delta$
- estimate the coverage probability and average width of a 95% confidence interval

Place all of these estimates in a data frame with columns

- `Method` (taking the value `Student T`, `Yuen T`, or `Median`)
- `n` and `delta`
- `Coverage Probability`, `SECoverage` (the estimated standard error of the coverage probability), `CovLCL` and `CovUCL` (the coverage probability plus or minus two standadrd errors)
- `Average Width` (the estimated average width of the confidence interval)

It should take under five minutes to generate all of these simulations. ***Note*** that the output of the `MedianCI` function is a *vector*, with the confidence limits being in the second and third positions of the vector.

```{r, eval=FALSE}
N <- 10^4

# Function to perform Monte Carlo simulations
simulate_intervals <- function(n, delta, method) {
  # Set seed for reproducibility
  set.seed(123)
  
  # Initialize empty vectors to store results
  coverage_prob <- numeric(0)
  avg_width <- numeric(0)
  
  # Perform Monte Carlo simulations
  for (i in 1:N) {
    # Generate sample from slash distribution
    sample_data <- rslash(n, delta)
    
    # Calculate confidence interval based on the method
    if (method == "Student T") {
      result <- t.test(sample_data)$conf.int
    } else if (method == "Yuen T") {
      result <- YuenTTest(sample_data)$conf.int
    } else if (method == "Median") {
      result <- MedianCI(sample_data)[c(2, 3)]
    }
    
    # Calculate coverage probability
    coverage_prob[i] <- result[1] <= 0 & 0 <= result[2]
    
    # Calculate width of the confidence interval
    avg_width[i] <- result[2] - result[1]
  }
  
  # Calculate summary statistics
  mean_coverage <- mean(coverage_prob)
  se_coverage <- sd(coverage_prob) / sqrt(N)
  coverage_lcl <- mean_coverage - 2 * se_coverage
  coverage_ucl <- mean_coverage + 2 * se_coverage
  
  mean_width <- mean(avg_width)
  
  # Create a data frame with results
  results_df <- data.frame(
    Method = method,
    n = n,
    delta = delta,
    Coverage_Probability = mean_coverage,
    SECoverage = se_coverage,
    CovLCL = coverage_lcl,
    CovUCL = coverage_ucl,
    Average_Width = mean_width
  )
  
  return(results_df)
}

# Set parameters for simulation
n_values <- c(20, 40, 60, 80, 100)
delta_values <- c(1, 1.5, 2, 2.5)
methods <- c("Student T", "Yuen T", "Median")

# Initialize an empty data frame
results <- data.frame()

# Perform simulations for all combinations
for (n in n_values) {
  for (delta in delta_values) {
    for (method in methods) {
      results <- bind_rows(results, simulate_intervals(n, delta, method))
    }
  }
}
```

```{r, eval=FALSE}
save(results, file = "intervals_df.Rdata")
```

```{r}
load("intervals_df.Rdata")
```

Once you have performed these simulations, create several graphics which allow you to deduce any patterns in the coverage probabilities or average width of these different methods. 

- Create a set of plots which plot the estimated coverage probability (with error bars) with sample size $n$ on the *x*-axis. Use different aesthetics to make it possible to visually compare the three different confidence interval methods, for each of the different values of $\delta$ (for instance, you might consider using `facet_grid` or `facet_wrap`). Include a horizontal reference line at the nominal confidence level 0.95 in each of these plots.

```{r}
# Plotting
ggplot(results, aes(x = n, y = Coverage_Probability, group = Method, color = Method)) +
  geom_errorbar(aes(ymin = CovLCL, ymax = CovUCL), width = 0.3) +
  geom_line() +
  geom_point() +
  facet_grid(delta ~ Method, scales = "free_y", labeller = label_both) +
  labs(
    title = "Estimated Coverage Probability with Error Bars",
    x = "Sample Size (n)",
    y = "Coverage Probability",
    color = "Method"
  ) +
  geom_hline(yintercept = 0.95, linetype = "dashed", color = "red") +
  theme_minimal()
```

- Create another set of plots which plot the estimated average width of each of the three types of confidence intervals with sample size $n$ on the *x*-axis. Use an aesthetic such as color (and/or plotting symbol `pch`, and/or line type `lty`) to map to the type of confidence interval, and create one plot for each value of `delta` (again, perhaps with the `facet_wrap()` function).

```{r}
# Plotting
ggplot(results, aes(x = n, y = Average_Width, group = Method, color = Method)) +
  geom_line() +
  geom_point() +
  facet_wrap(~ delta, scales = "free_y", labeller = label_both) +
  labs(
    title = "Estimated Average Width of Confidence Intervals",
    x = "Sample Size (n)",
    y = "Average Width",
    color = "Method"
  ) +
  theme_minimal()
```

Make a set of comments on the coverage probabilities and average widths. Does one method appear to be better than the others? How does the shape parameter $\delta$ affect these comparisons?

**The Yuen T method typically produces narrower confidence intervals, whereas the Student T method yields the widest intervals. The median method tends to closely align with what the Yuen T method outputs. As the delta increases, the disparity between the outputs of these three methods becomes smaller.**

## Comparing Tests of Homoscedasticity

There are a variety of hypothesis testing procedures designed to test the null hypothesis that two (or more) populations have the same population variance. You will focus on comparing the size and power of two of these hypotheses tests:

- the *F*-test for comparing two population variances from normal populations
- the Fligner-Killeen test

Either of these tests can be viewed as testing the hypotheses
$$H_0\colon\frac{\sigma_1}{\sigma_2}=1\quad\text{against}\quad H_a\colon\frac{\sigma_1}{\sigma_2}\ne 1$$
where $\sigma_1$ and $\sigma_2$ are the population standard deviations of the two sampled populations. 

For each combination of...

- sample sizes $n\in\{20, 40, 60\}$
- ratio of population standard deviations $r=\sigma_1/\sigma_2\in\{1,1.1,1.2,\dotsc,2\}$
- three different populations (normal, exponential, and gamma)
- two hypothesis testing procedures (*F*-test, or Fligner-Killeen test)

you should estimate the probability of rejecting the null hypothesis with Monte Carlo simulations. Use only $N=5000$ replicates for these estimates (a bit better than only $10^3$ replicates, but not quite as time-consuming as generating $10^4$ replicates). It should take around 10 minutes to generate all of these samples.

For each replicate, you must generate two samples each of size $n$ from their respective populations:

- For sampling from the normal population, one sample should come from the standard normal distribution, and the second sample should come from a normal distribution with `mean=0` and `sd=r` (*r* being the ratio of population standard deviations that is allowed to vary)
- For sampling from the exponential population, one sample should come from the exponential distribution with rate parameter 1, and the second sample should come from the exponential distribution with rate parameter *r*
- For samping from the gamma population, one sample should come from the gamma distribution with shape parameter $\alpha=5$ and rate parameter 1, and the second sample should come from the gamma distribution with shape parameter $\alpha=5$ and rate parameter *r*

To obtain a *p*-value from the *F*-test, note that you may use the `var.test()` function. For instance,
```{r}
var.test(rnorm(60), rnorm(60, sd = 1.4))
```
performs the *F*-test on a sample of $n_1=60$ observations from the standard normal distribution and a sample of $n_2=60$ observations from the normal distribution with mean 0 and standard deviation 1.4 (so that $r=\sigma_2/\sigma_1=1.4/1=1.4$). To extract the *p*-value from this testing object, use the `$p.value` extraction operator. 

To implement the Fligner-Killeen test, you may use the `fligner.test()` function. This requires the two samples to be input into the function as a *list*:
```{r}
fligner.test(list(x = rnorm(60), y = rnorm(60, sd = 1.4)))
```
Again, you may use `$p.value` to extract the *p*-value from this output. 

Generate a data frame whose columns consist of...

- `Test`, taking the values `F Test` or `Fligner-Killeen`
- `Population`, taking the values `Normal`, `Exponential`, and `Gamma (shape=5)`
- `Power`, the estimated probability of rejecting the null hypothesis when using a significance level of $\alpha=0.05$
- `PowerSE`, an estimate of the standard error of the power
- `PowerLCL` and `PowerUCL`, the power plus or minus two estimated standard errors

```{r, eval=FALSE}
alpha <- 0.05

# Set seed for reproducibility
set.seed(123)

# Number of replicates
N <- 5000

# Function to perform Monte Carlo simulations
simulate_homoscedasticity_tests <- function(n, r, population, test) {
  # Initialize empty vectors to store results
  power <- numeric(0)
  
  # Perform Monte Carlo simulations
  for (i in 1:N) {
    # Generate samples based on the population type
    if (population == "Normal") {
      sample1 <- rnorm(n)
      sample2 <- rnorm(n, sd = r)
    } else if (population == "Exponential") {
      sample1 <- rexp(n)
      sample2 <- rexp(n, rate = r)
    } else if (population == "Gamma (shape=5)") {
      sample1 <- rgamma(n, shape = 5)
      sample2 <- rgamma(n, shape = 5, rate = r)
    }
    
    # Perform the test and extract p-value
    if (test == "F Test") {
      p_value <- var.test(sample1, sample2)$p.value
    } else if (test == "Fligner-Killeen") {
      p_value <- fligner.test(list(x = sample1, y = sample2))$p.value
    }
    
    # Determine if null hypothesis is rejected (power calculation)
    reject_null <- p_value < alpha
    power[i] <- as.numeric(reject_null)
  }
  
  # Calculate summary statistics
  mean_power <- mean(power)
  se_power <- sd(power) / sqrt(N)
  power_lcl <- mean_power - 2 * se_power
  power_ucl <- mean_power + 2 * se_power
  
  # Create a data frame with results
  results_df <- data.frame(
    n = n,
    r = r,
    Test = test,
    Population = population,
    Power = mean_power,
    PowerSE = se_power,
    PowerLCL = power_lcl,
    PowerUCL = power_ucl
  )
  
  return(results_df)
}

# Set parameters for simulation
n_values <- c(20, 40, 60)
r_values <- seq(1, 2, by = 0.1)
populations <- c("Normal", "Exponential", "Gamma (shape=5)")
tests <- c("F Test", "Fligner-Killeen")

# Initialize an empty data frame
results <- data.frame()

# Perform simulations for all combinations
for (n in n_values) {
  for (r in r_values) {
    for (population in populations) {
      for (test in tests) {
        results <- bind_rows(results, simulate_homoscedasticity_tests(n, r, population, test))
      }
    }
  }
}
```

```{r, eval=FALSE}
save(results, file = "hypothesis_df.Rdata")
```

```{r}
load("hypothesis_df.Rdata")
```

Investigate properties of these two hypothesis tests with appropriate tables or graphics:

- The *size* of the test is the probability of rejecting $H_0$ when $r=1$ (i.e. when $H_0$ is true). Create a graphical display which plots the estimated size (with error bars also plotted) with sample size $n$ along the *x*-axis. Use a color/plotting symbol/line type aesthetic to indicate the type of test (*F* or Fligner-Killeen), and a call to `facet_wrap()` to create separate plots for each of the three different sampled populations. Also include a horizontal reference line at $\alpha=0.05$ in each of these plots.

```{r}
# Filter results for size analysis (r = 1)
size_data <- results %>% filter(r == 1)

# Create the plot
size_plot <- ggplot(size_data, aes(x = as.factor(n), y = Power, color = Test, linetype = Test)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(
    aes(ymin = PowerLCL, ymax = PowerUCL),
    position = position_dodge(width = 0.5),
    width = 0.2
  ) +
  facet_wrap(~Population, scales = "fixed") +
  geom_hline(yintercept = alpha, linetype = "dashed", color = "red") +
  labs(
    title = "Size of Homoscedasticity Tests",
    x = "Sample Size (n)",
    y = "Power",
    color = "Test",
    linetype = "Test"
  ) +
  theme_minimal()

# Display the plot
print(size_plot)
```

- Generate power curves with error bars which compare the power of these two tests. Power should be on the *y*-axis, *r* should be plotted along the *x*-axis, and a color/plotting symbol/line type aesthetic should be mapped to the different type of test. Create nine different plots (the `facet_grid()` function is likely useful here) of these power curves, one for each combination of $n$ (20, 40, or 60) and the sampled population (normal, exponential, or gamma).

```{r}
# Create power curves
power_plot <- ggplot(results, aes(x = r, y = Power, color = Test, linetype = Test)) +
  geom_point(position = position_dodge(width = 0.1), size = 3) +
  geom_errorbar(
    aes(ymin = PowerLCL, ymax = PowerUCL),
    position = position_dodge(width = 0.1),
    width = 0.02
  ) +
  facet_grid(n ~ Population) +
  labs(
    title = "Power Comparison of Homoscedasticity Tests",
    x = "Ratio of Population Standard Deviations (r)",
    y = "Power",
    color = "Test",
    linetype = "Test"
  ) +
  theme_minimal()

# Display the power curves
print(power_plot)
```

Make a set of comments on these two sets of plots. Does the Type I error probability (size) of either test agree with the nominal significance level $\alpha=0.05$ for all $n$ and populations? Does one test appear to be better than the other in terms of having a size close to 0.05 (and under what conditions)?

**Type one error of the Fligner-Killeen test mostly tends to agree with the nominal significance level (at least for gamma and normal populations. The F test does well only on the normal population. The Fligner-Killeen test mostly outperforms the F test (except for normal population, where the F test is slightly better when n is small).**

When comparing the power curves for the two hypothesis tests, which one tends to be better (under which circumstances)? How do any perceived differences change with the sample size $n$?

**The power of F test is generally higher. When n is high and r is high, the powers of these two tests are almost similar. For gamma and normal populations, the powers of these tests align pretty well when n is large.**